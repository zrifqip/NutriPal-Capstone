# -*- coding: utf-8 -*-
"""BMI_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l_dpdSnLFqn7H5KBBNU-KzDMaqjvIe60
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import tensorflow as tf
import pickle
from mappings import gender_mapping, family_history_mapping, favc_mapping, caec_mapping, smoke_mapping, scc_mapping, calc_mapping, mtrans_mapping

# Read the dataset from a CSV file
df = pd.read_csv('weight_encoded_2.csv')

# Calculate BMI column
df['BMI'] = df['Weight'] / (df['Height'] ** 2)

# Separate features and target variable
X = df.drop('NObeyesdad', axis=1)
y = df['NObeyesdad']

# Apply mappings to categorical columns
X['Gender'] = X['Gender'].map(gender_mapping)
X['family_history_with_overweight'] = X['family_history_with_overweight'].map(family_history_mapping)
X['FAVC'] = X['FAVC'].map(favc_mapping)
X['CAEC'] = X['CAEC'].map(caec_mapping)
X['SMOKE'] = X['SMOKE'].map(smoke_mapping)
X['SCC'] = X['SCC'].map(scc_mapping)
X['CALC'] = X['CALC'].map(calc_mapping)
X['MTRANS'] = X['MTRANS'].map(mtrans_mapping)

# Encode categorical columns using OneHotEncoder
categorical_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']
preprocessor = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_cols)], remainder='passthrough')
X = preprocessor.fit_transform(X)

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Set the random seed for reproducibility
tf.random.set_seed(42)

# Build the model architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')
])

# Compile the model
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(lr=0.001),
    metrics=['accuracy']
)

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_test_scaled, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)

# Print the evaluation results
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Save the model as a .pkl file
with open('BMI_2.pkl', 'wb') as file:
    pickle.dump(model, file)